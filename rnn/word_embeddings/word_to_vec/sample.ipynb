{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data(data_path:str):\n",
    "    with open(data_path,'r') as fp:\n",
    "        data=fp.read()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def preprocess_data(text:str):\n",
    "    text=text.lower()\n",
    "    words=text.split()\n",
    "    word_counts=Counter(words)\n",
    "    filtered_words=[word for word in words if word_counts[word]>10]\n",
    "    return filtered_words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_data(\"/home/local/ZOHOCORP/muikumar-pt6527/Desktop/Deep_learning/rnn/word_embeddings/word_to_vec/data/datasets/text8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=preprocess_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(words):\n",
    "    word_counts=Counter(words)\n",
    "    sorted_vocab=sorted(word_counts,key=word_counts.get,reverse=True)\n",
    "    word_to_int={word:idx for idx,word in enumerate(sorted_vocab)}\n",
    "    int_to_word={idx:word for word,idx in word_to_int.items()}\n",
    "    return word_to_int,int_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "def sub_sampling(int_words,threshold=1e-5):\n",
    "    word_counts=Counter(int_words)\n",
    "    total_words=len(int_words)\n",
    "    word_freq_ratios={word:freq/total_words for word,freq in word_counts.items()}   \n",
    "    p_drop={word:1-np.sqrt(threshold/word_freq_ratios[word]) for word in word_counts}\n",
    "    rand_prob=random.random()\n",
    "    return [word for word in int_words if rand_prob>p_drop[word]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(int_words,idx,max_window_size):\n",
    "    window_size=random.randint(1,max_window_size)\n",
    "    start=max(0,idx-window_size)\n",
    "    end=min(idx+window_size+1,len(int_words)-1)\n",
    "    context_words=int_words[start:idx]+int_words[idx+1:end]\n",
    "    return context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(int_words,batch_size,max_window_size):\n",
    "    n_batches=len(int_words)//batch_size\n",
    "    int_words=int_words[:n_batches*batch_size]\n",
    "    for batch_num in range(0,len(int_words),batch_size):\n",
    "        batch_words=int_words[batch_num:batch_num+batch_size]\n",
    "        \n",
    "        batch_x,batch_y=[],[]\n",
    "        for i in range(len(batch_words)):\n",
    "            target_word=batch_words[i]\n",
    "            context_words=get_context(batch_words,i,max_window_size)\n",
    "            batch_x.extend([target_word]*len(context_words))\n",
    "            batch_y.extend(context_words)\n",
    "        yield batch_x,batch_y\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "class SkipGramNegSampling(nn.Module):\n",
    "    def __init__(self,n_vocab,n_embed,noise_dist):\n",
    "        super(SkipGramNegSampling,self).__init__()\n",
    "        self.n_vocab=n_vocab\n",
    "        self.n_embed=n_embed\n",
    "        self.noise_dist=noise_dist\n",
    "        \n",
    "        self.context_embed=nn.Embedding(n_vocab,n_embed)\n",
    "        self.target_embed=nn.Embedding(n_vocab,n_embed)\n",
    "        \n",
    "        self.context_embed.weight.data.uniform_(-1,1)\n",
    "        self.target_embed.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def forward_context(self,contexts):\n",
    "        embed_contexts=self.context_embed(contexts)\n",
    "        return embed_contexts\n",
    "    \n",
    "    def forward_target(self,target):\n",
    "        embed_target=self.target_embed(target)\n",
    "        return embed_target\n",
    "    \n",
    "    def forward_noise(self,batch_size,n_samples):\n",
    "        if self.noise_dist:\n",
    "            noise=self.noise_dist\n",
    "        else:\n",
    "            noise=torch.ones(self.n_vocab)\n",
    "        \n",
    "        noise_words=torch.multinomial(noise,\n",
    "                                  num_samples=n_samples,\n",
    "                              replacement=True)\n",
    "        noise_words.to(device)\n",
    "        noise_vector=self.target_embed(noise_words).view(batch_size,n_samples,self.n_embed)\n",
    "        return noise_vector\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSamplingLoss(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(NegativeSamplingLoss, self).__init__()\n",
    "    def forward(self,\n",
    "            target_vector,\n",
    "            context_vector,\n",
    "            noise_vector):\n",
    "        batch_size,embedding_size=target_vector.shape\n",
    "        target_vector=target_vector.view(batch_size,embedding_size,1)\n",
    "        context_vector=context_vector.view(batch_size,1,embedding_size)\n",
    "        out_loss=torch.bmm(context_vector,target_vector).sigmoid().log().squeeze()\n",
    "        \n",
    "        noise_loss=torch.bmm(noise_vector.neg(),target_vector).sigmoid().log()\n",
    "        noise_loss=noise_loss.squeeze().sum(1)\n",
    "        return -(out_loss+noise_loss).mean()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "woi,iow=build_vocab(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_words=[woi[word] for word in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_words=sub_sampling(int_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=get_batches(int_words,8,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b=next(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.5537e-02, 1.0049e-02, 7.7052e-03,  ..., 2.8380e-06, 2.8380e-06,\n",
      "        2.8380e-06], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "freq = Counter(train_data)\n",
    "freq_ratio = {word:cnt/len(woi) for word, cnt in freq.items()}     \n",
    "freq_ratio = np.array(sorted(freq_ratio.values(), reverse=True))\n",
    "unigram_dist = freq_ratio / freq_ratio.sum() \n",
    "noise_dist = torch.from_numpy(unigram_dist**0.75 / np.sum(unigram_dist**0.75))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
